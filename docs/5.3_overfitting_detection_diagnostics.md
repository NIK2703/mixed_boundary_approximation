## Подшаг 5.3: Проверка на переобучение — подробный план реализации на C++

### Шаг 5.3.1: Теоретические основы переобучения в полиномиальной аппроксимации

**Механизмы возникновения переобучения:**
- **Феномен Рунге:** Осцилляции полинома высокой степени на границах интервала при недостаточной регуляризации
- **Чувствительность к выбросам:** Сильное влияние отдельных точек с малыми весами `σ_i` на форму полинома
- **Конфликт критериев:** Противоречие между аппроксимацией данных и отталкиванием от запрещённых областей приводит к «зигзагообразному» поведению между барьерами
- **Недостаточная регуляризация:** Слишком малый параметр `γ` не подавляет высокочастотные компоненты полинома

**Характерные признаки переобучения:**
- Высокая норма второй производной `||F''||_L2` при хорошем значении аппроксимирующего члена
- Экстремальные осцилляции между узлами данных (локальные максимумы/минимумы без физического обоснования)
- Чувствительность решения к малым изменениям входных данных (плохая обусловленность)
- Большой разрыв между ошибкой на обучающих точках и ошибкой на контрольных точках (при наличии)

### Шаг 5.3.2: Метрика 1 — Анализ нормы второй производной

**Математическая основа:**
- Регуляризационный член `γ·∫(F''(x))²dx` напрямую контролирует «энергию кривизны»
- Абсолютное значение нормы малоинформативно — требуется нормировка на характерный масштаб задачи

**Алгоритм вычисления нормированной нормы кривизны:**
1. **Вычисление абсолютной нормы:**
   ```
   ||F''||² = ∫_a^b [F''(x)]² dx
   ```
   - Для полинома вычисляется аналитически через коэффициенты (шаг 3.2.4, вариант A)
   - Альтернатива: численное интегрирование методом Гаусса с 50 узлами

2. **Оценка характерного масштаба кривизны:**
   ```
   scale_curvature_expected = max|f(x_i)| / (b - a)²
   ```
   - Представляет «естественную» кривизну для гладкой функции, проходящей через данные
   - Для линейной функции кривизна = 0; для параболы ~ `амплитуда / (диапазон)²`

3. **Нормированная метрика:**
   ```
   κ = ||F''|| / scale_curvature_expected
   ```
   - Интерпретация:
     * `κ < 1` — решение гладкое, регуляризация эффективна
     * `1 ≤ κ < 10` — умеренная кривизна, допустимо для сложных данных
     * `κ ≥ 10` — признак переобучения или избыточной сложности модели
     * `κ ≥ 100` — сильное переобучение, требуется коррекция

4. **Адаптивный порог на основе данных:**
   - Учёт плотности данных: `ρ = N_approx / (b - a)`
   - Коррекция порога: `κ_threshold = 5.0 · max(1.0, log10(1.0 + n / ρ))`
   - При разреженных данных допускается большая кривизна

### Шаг 5.3.3: Метрика 2 — Обнаружение осцилляций через анализ экстремумов

**Принцип обнаружения:**
- Полином степени `n` может иметь до `n-1` локальных экстремумов
- Естественное распределение экстремумов: концентрация в областях высокой вариации данных
- При переобучении: экстремумы появляются в «пустых» областях между точками данных

**Алгоритм обнаружения осцилляций:**

1. **Поиск критических точек (корней первой производной):**
   - Для полинома `F'(x)` степени `n-1` найти все вещественные корни на `[a, b]`
   - Метод: комбинация исчерпывающего поиска на равномерной сетке + уточнение методом Ньютона
     ```
     сетка: x_grid[k] = a + k·Δx, где Δx = (b-a)/1000
     для каждого интервала [x_grid[k], x_grid[k+1]]:
         если sign(F'(x_grid[k])) ≠ sign(F'(x_grid[k+1])):
             применить метод Ньютона для уточнения корня
     ```

2. **Классификация экстремумов:**
   - Для каждого найденного корня `x_ext`:
     * Определить тип: минимум (`F''(x_ext) > 0`) или максимум (`F''(x_ext) < 0`)
     * Найти ближайшие точки данных: `d_left = min|x_ext - x_i|` для `x_i < x_ext`
     * Оценить «пустоту» окрестности: `gap_ratio = min(d_left, d_right) / median_spacing`
       где `median_spacing` — медианное расстояние между соседними точками данных

3. **Метрика осцилляций:**
   ```
   oscillation_score = Σ_{экстремумы в пустых областях} (1.0 / (1.0 + gap_ratio))
   где «пустая область» определяется как gap_ratio > 3.0
   ```
   - Интерпретация:
     * `oscillation_score < 0.5` — осцилляции отсутствуют или незначительны
     * `0.5 ≤ oscillation_score < 2.0` — умеренные осцилляции, требуют внимания
     * `oscillation_score ≥ 2.0` — сильные осцилляции, признак переобучения

4. **Визуальная верификация (опционально для отладки):**
   - Генерация графика с отметками экстремумов и точек данных
   - Цветовая кодировка: зелёные (естественные), жёлтые (подозрительные), красные (осцилляции)

### Шаг 5.3.4: Метрика 3 — Кросс-валидация с исключением точек

**Стратегия кросс-валидации для задачи с ограничениями:**
- Проблема: стандартная кросс-валидация нарушает интерполяционные условия
- Решение: модифицированная схема, сохраняющая обязательные узлы

**Алгоритм модифицированной кросс-валидации:**

1. **Формирование фолдов:**
   - Интерполяционные узлы `(z_e)` всегда включаются во все фолды (обязательные точки)
   - Аппроксимирующие точки `(x_i)` разбиваются на `K = min(5, N_approx)` фолдов
   - Отталкивающие точки `(y_j)` дублируются во все фолды (барьеры должны действовать всегда)

2. **Цикл кросс-валидации:**
   ```
   для каждого фолда k = 1..K:
       обучающее_множество = все точки кроме фолда k + все интерполяционные узлы
       валидационное_множество = фолд k
       
       временно_оптимизировать_полином(обучающее_множество)
       вычислить_ошибку = Σ_{i∈валидационное} |f(x_i) - F(x_i)|² / σ_i
       накопить_ошибку += ошибка
   ```
   - Важно: параметры регуляризации `γ` и веса барьеров `B_j` фиксированы на всём цикле

3. **Метрика обобщающей способности:**
   ```
   CV_score = sqrt( накопленная_ошибка / N_approx )
   train_error = RMS остатков на полном наборе данных
   обобщающая_способность = CV_score / train_error
   ```
   - Интерпретация:
     * `обобщающая_способность < 1.5` — хорошая обобщающая способность
     * `1.5 ≤ обобщающая_способность < 3.0` — умеренное переобучение
     * `обобщающая_способность ≥ 3.0` — сильное переобучение

4. **Оптимизация производительности:**
   - Для больших наборов (`N_approx > 100`) использовать 3-кратную кросс-валидацию вместо 5-кратной
   - Кэшировать результаты оптимизации для близких фолдов (при малом числе точек в фолде)
   - Параллельное выполнение фолдов через `std::async` (один поток на фолд)

### Шаг 5.3.5: Метрика 4 — Анализ чувствительности к шуму

**Принцип метода:**
- Переобученная модель чрезмерно чувствительна к малым возмущениям входных данных
- Измерение изменения выхода при добавлении контролируемого шума к входным точкам

**Алгоритм анализа чувствительности:**

1. **Генерация возмущённых наборов:**
   - Создать `M = 10` возмущённых копий аппроксимирующих точек:
     ```
     для копии m = 1..M:
         для каждой точки i:
             x_i^{(m)} = x_i + δ_x · randn()
             f_i^{(m)} = f_i + δ_y · randn()
         где randn() ~ N(0,1), δ_x = 1e-4·(b-a), δ_y = 1e-3·std(f)
     ```

2. **Оценка вариации решения:**
   - Для каждой копии вычислить полином с фиксированными параметрами оптимизации
     (без повторной оптимизации — только оценка чувствительности к данным)
   - Вычислить стандартное отклонение значений полинома на контрольной сетке:
     ```
     сетка: ξ_p = a + p·(b-a)/100, p = 0..100
     для каждой точки сетки ξ_p:
         значения_F[p][m] = F^{(m)}(ξ_p)
         std_F[p] = std(значения_F[p][1..M])
     ```

3. **Метрика чувствительности:**
   ```
   sensitivity_score = max_p(std_F[p]) / max_p|F(ξ_p)|
   ```
   - Интерпретация:
     * `sensitivity_score < 0.01` — устойчивое решение
     * `0.01 ≤ sensitivity_score < 0.1` — умеренная чувствительность
     * `sensitivity_score ≥ 0.1` — высокая чувствительность, признак переобучения

### Шаг 5.3.6: Комплексная оценка и принятие решения

**Интеграция метрик в единую оценку риска переобучения:**
```
risk_score = w1·normalize(κ) + w2·normalize(oscillation_score) 
           + w3·normalize(обобщающая_способность) + w4·normalize(sensitivity_score)

где normalize(metric) = (metric - threshold_low) / (threshold_high - threshold_low)
      clipped to [0, 1]

веса: w1 = 0.4 (кривизна — основной индикатор)
      w2 = 0.3 (осцилляции — визуальный признак)
      w3 = 0.2 (кросс-валидация — обобщающая способность)
      w4 = 0.1 (чувствительность — дополнительный индикатор)
```

**Пороги принятия решения:**
- `risk_score < 0.3` — решение качественное, переобучение отсутствует
- `0.3 ≤ risk_score < 0.7` — умеренное переобучение, рекомендуется коррекция
- `risk_score ≥ 0.7` — сильное переобучение, требуется обязательная коррекция

**Генерация диагностического отчёта:**
```
Диагностика переобучения:
  • Нормированная кривизна: κ = 12.3 (порог: 10.0) → ПРЕВЫШЕНИЕ
  • Осцилляции: 3 экстремума в пустых областях → УМЕРЕННО
  • Обобщающая способность: 2.1 (порог: 1.5) → ПРЕВЫШЕНИЕ
  • Чувствительность к шуму: 0.08 → НОРМАЛЬНО
  
  Итоговый риск переобучения: 0.65/1.0 (УМЕРЕННЫЙ)
  
  Рекомендации:
    1. Увеличить параметр регуляризации γ с 0.1 до 0.5
    2. Рассмотреть снижение степени полинома с 15 до 10
    3. Проверить наличие выбросов в аппроксимирующих точках
```

### Шаг 5.3.7: Адаптивные стратегии коррекции переобучения

**Стратегия A: Усиление регуляризации (первичная коррекция):**
1. **Автоматическая коррекция параметра `γ`:**
   ```
   γ_new = γ_current · (1.0 + α · risk_score)
   где α = 2.0 — коэффициент усиления
   
   Ограничения:
     γ_new ≤ γ_max = 100.0 · γ_initial  // избежать чрезмерного сглаживания
     γ_new ≥ γ_min = 1e-6               // сохранить минимальную регуляризацию
   ```

2. **Итеративная коррекция с переоптимизацией:**
   - Применить новое значение `γ_new`
   - Выполнить дополнительные итерации оптимизации (до 50 итераций)
   - Повторно оценить `risk_score`
   - При `risk_score < 0.3` — завершить коррекцию
   - При `risk_score ≥ 0.7` после 3 итераций — перейти к стратегии Б

**Стратегия Б: Снижение сложности модели:**
1. **Адаптивное снижение степени полинома:**
   ```
   Δn = max(1, floor(0.2 · n_current · risk_score))
   n_new = max(n_min, n_current - Δn)
   где n_min = m  // минимальная степень = число интерполяционных узлов
   ```

2. **Переинициализация с новой степенью:**
   - Сохранить текущее решение как «эталон»
   - Построить полином степени `n_new` через проекцию:
     * Вычислить значения текущего полинома в `n_new + 1` точках на `[a, b]`
     * Решить интерполяционную задачу для получения коэффициентов новой степени
   - Применить коррекцию барьеров (шаг 4.2.3) к новому полиному
   - Выполнить полную оптимизацию с новой степенью

**Стратегия В: Коррекция весов данных (обработка выбросов):**
1. **Обнаружение потенциальных выбросов:**
   - Вычислить стандартизованные остатки: `r_i = (f(x_i) - F(x_i)) / (σ_i · RMS_residual)`
   - Точки с `|r_i| > 3.0` классифицировать как потенциальные выбросы

2. **Адаптивная коррекция весов:**
   ```
   для каждой точки i:
       если |r_i| > 3.0:
           σ_i_new = σ_i · (1.0 + β · (|r_i| - 3.0))
           где β = 0.5 — коэффициент ослабления влияния
   ```
   - Это уменьшает влияние выбросов на форму полинома без их полного исключения

### Шаг 5.3.8: Интеграция в основной цикл оптимизации

**Точка внедрения проверки:**
- Выполнять диагностику переобучения **после сходимости основной оптимизации** (шаг 4.3)
- Дополнительно: периодическая проверка каждые 100 итераций для раннего обнаружения проблем

**Алгоритм адаптивной оптимизации с диагностикой:**
```
1. Выполнить базовую оптимизацию до сходимости
2. Оценить risk_score переобучения
3. ЕСЛИ risk_score ≥ 0.7:
       применить стратегию А (усиление γ)
       повторить оптимизацию
       перейти к шагу 2
   ИНАЧЕ ЕСЛИ risk_score ≥ 0.3:
       выдать предупреждение пользователю
       предложить ручную коррекцию параметров
       продолжить с текущим решением
   ИНАЧЕ:
       принять решение как качественное
4. Вернуть финальное решение с диагностическим отчётом
```

**Обработка ложных срабатываний:**
- При наличии физически обоснованных осцилляций в данных (например, периодические сигналы):
  * Пользователь может пометить данные как «осциллирующие» через флаг в конфигурации
  * Система снижает вес метрики `oscillation_score` до 0.05
  * Основной фокус смещается на кросс-валидацию и чувствительность

**Логирование для аудита:**
- Сохранять историю всех коррекций:
  * Исходные параметры (`n`, `γ`, `B_j`)
  * Значения метрик переобучения до/после коррекции
  * Применённые изменения параметров
  * Итоговое значение функционала
- Это позволяет воспроизвести процесс оптимизации и обосновать принятые решения

Этот план обеспечивает комплексную, многоуровневую диагностику переобучения с количественными метриками и адаптивными стратегиями коррекции. Подход учитывает специфику задачи смешанной аппроксимации (барьеры, интерполяционные ограничения) и предоставляет прозрачную обратную связь пользователю для принятия обоснованных решений о качестве полученного решения.