## Подшаг 4.1: Выбор и настройка оптимизатора — подробный план реализации на C++

### Шаг 4.1.1: Анализ характеристик задачи для выбора алгоритма

**Классификация задачи оптимизации:**
- **Тип:** Безусловная нелинейная оптимизация (благодаря параметризации шага 2.1 интерполяционные ограничения удовлетворяются автоматически)
- **Размерность:** `n_free = n - m + 1` свободных параметров (обычно 2–50 для практических задач)
- **Выпуклость:** Функционал **невыпуклый** из-за отталкивающего члена с барьерами — возможны множественные локальные минимумы
- **Гладкость:** Функционал дважды дифференцируем в допустимой области, но имеет особенности вблизи барьеров (`|y_j^* - F(y_j)| → 0`)
- **Численная устойчивость:** Требует защиты от деления на ноль и взрывного роста градиента в критических зонах

**Критерии выбора оптимизатора в зависимости от размерности:**
- **Низкая размерность (`n_free ≤ 5`):**
  * Метод Нелдера-Мида (симплекс) — безградиентный, устойчив к шуму
  * Градиентный спуск с адаптивным шагом — простота реализации
  
- **Средняя размерность (`5 < n_free ≤ 50`):**
  * **L-BFGS-B (рекомендуемый)** — квазиньютоновский метод с ограниченной памятью
    - Требует только градиент (не гессиан)
    - Эффективен для гладких функций
    - Поддерживает ограничения на переменные (полезно для стабилизации)
    - Доступен через библиотеки: `NLopt` (алгоритм `LD_LBFGS`), `dlib::optimization`, или собственная реализация
  
- **Высокая размерность (`n_free > 50`):**
  * L-BFGS без ограничений (меньше накладных расходов)
  * Стохастические методы (если допустимо снижение точности)

**Обоснование выбора L-BFGS-B как основного алгоритма:**
- Баланс между скоростью сходимости (суперлинейная) и вычислительной сложностью O(n_free)
- Устойчивость к неточностям градиента (важно при барьерной защите)
- Возможность задания ограничений на коэффициенты для предотвращения численных аномалий
- Широкая проверка на практике в задачах регрессии и аппроксимации

### Шаг 4.1.2: Интеграция оптимизатора с вычислительным ядром

**Архитектура интерфейса «оптимизатор ↔ вычислительное ядро»:**
- Создать класс-адаптер `OptimizationProblem` реализующий интерфейс, ожидаемый оптимизатором:
  * Метод `evaluate(const std::vector<double>& params)` — вычисление функционала `J`
  * Метод `evaluate_with_gradient(const std::vector<double>& params, std::vector<double>& grad)` — совместное вычисление функционала и градиента (оптимизация кэша)
  * Метод `get_dimension()` — возврат `n_free`
  * Метод `get_bounds()` — опциональные ограничения на параметры (для стабилизации)

**Оптимизация вызовов функционала/градиента:**
- **Единый вызов для функционала и градиента:** Избежать двойного вычисления общих подвыражений (значений полинома в точках данных)
- **Кэширование между вызовами:**
  * После вычисления `F(x_i)`, `F(y_j)` сохранить значения до следующего изменения параметров
  * При вызове градиента использовать закэшированные значения вместо повторного вычисления
- **Ленивая инвалидация кэша:** Кэш помечается как «устаревший» только при фактическом изменении параметров

**Обработка численных аномалий в интеграционном слое:**
- Перехват исключений из вычислительного ядра (деление на ноль, переполнение)
- При обнаружении аномалии:
  * Вернуть искусственно большое значение функционала (`1e20`)
  * Вернуть градиент, направленный «от опасной области» (на основе предыдущего безопасного градиента)
  * Зафиксировать событие в логе для диагностики
- Это предотвращает аварийное завершение оптимизатора при временных численных проблемах

### Шаг 4.1.3: Стратегия инициализации параметров

**Базовая стратегия (рекомендуемая):**
1. **Построение начального приближения через взвешенный МНК:**
   - Решить задачу минимизации только аппроксимирующего члена (игнорируя отталкивание и регуляризацию)
   - Получить коэффициенты полинома `a_k^LS`
   - Преобразовать в свободные параметры `q_k` через проекцию на подпространство `Q(x)·W(x)`
   - Преимущество: начальное приближение уже близко к данным, ускоряет сходимость

2. **Коррекция для избежания барьеров:**
   - Проверить расстояния до всех запрещённых точек: `dist_j = |y_j^* - F(y_j)|`
   - Если `dist_j < 5·ε_safe` для некоторой точки:
     * Применить локальную коррекцию коэффициентов через один шаг градиентного подъёма по отталкивающему члену
     * Или сдвинуть начальное приближение в направлении, увеличивающем минимальное расстояние до барьеров

**Альтернативные стратегии инициализации:**
- **Нулевая инициализация (`q_k = 0`):** `F(x) = P_int(x)` — безопасно, но может быть далеко от оптимума
- **Случайная инициализация:** Генерация `q_k` из нормального распределения `N(0, σ_init)`
  * Полезно для многократных запусков с целью поиска глобального минимума
  * `σ_init` подбирается пропорционально характерному масштабу данных: `σ_init = 0.1 · max|f(x_i)| / max|x_i|^n`

**Адаптивный выбор стратегии:**
- Если `N_approx > 10·n_free` (достаточно данных для надёжной МНК-оценки) → использовать МНК-инициализацию
- Если присутствуют сильные барьеры (`max(B_j) > 100·avg(σ_i)`) → добавить коррекцию от барьеров
- Если `n_free > 20` → комбинировать МНК с небольшим случайным возмущением для избежания плохих локальных минимумов

### Шаг 4.1.4: Настройка гиперпараметров оптимизатора L-BFGS-B

**Ключевые параметры и их настройка:**

1. **Параметр памяти `m_lbfgs` (количество сохраняемых коррекций):**
   - Рекомендуемое значение: `min(10, n_free)`
   - Для `n_free ≤ 5` → `m_lbfgs = n_free` (полная информация о кривизне)
   - Для `n_free > 20` → `m_lbfgs = 10` (баланс памяти/качества)

2. **Начальный шаг линейного поиска `α_initial`:**
   - Базовое значение: `1.0`
   - Адаптация при близости к барьерам:
     ```
     если min_dist_to_barrier < 2·ε_safe:
         α_initial = 0.1  // уменьшить шаг для осторожного подхода к барьеру
     ```

3. **Параметры линейного поиска (условие Вольфе):**
   - `c1 = 1e-4` (условие достаточного убывания)
   - `c2 = 0.9` (условие кривизны)
   - Максимальное число попыток линейного поиска: `20`

4. **Ограничения на параметры (для стабилизации):**
   - Динамические границы: `|q_k| ≤ Q_max`
   - `Q_max` подбирается на основе характерного масштаба:
     ```
     Q_max = 10 · max|f(x_i)| / min|W(x_i)|  // оценка максимального «вклада» коррекции
     ```
   - Предотвращает уход коэффициентов в область численной неустойчивости

### Шаг 4.1.5: Критерии останова и диагностика сходимости

**Основные критерии останова (логическое ИЛИ):**

1. **Относительное изменение функционала:**
   ```
   |J_k - J_{k-1}| / max(|J_k|, 1.0) < ε_J = 1e-8
   ```
   - Проверять по скользящему окну из 3 последних итераций для устойчивости к осцилляциям

2. **Норма градиента:**
   ```
   ||∇J||_2 / max(||∇J_initial||_2, 1.0) < ε_grad = 1e-6
   ```
   - Нормировка на начальную норму градиента обеспечивает масштабируемость критерия

3. **Максимальное число итераций:**
   - Базовое значение: `max_iter = 1000`
   - Адаптация: `max_iter = max(100, 20 · n_free)` для задач высокой размерности

4. **Таймаут по времени:**
   - Максимальное время: `timeout = 300 секунд` (настраиваемый параметр)
   - Проверка каждые 10 итераций для минимизации накладных расходов

**Диагностика проблем сходимости:**

1. **Осцилляции функционала:**
   - Обнаружение: стандартное отклонение `J` по последним 10 итерациям > `0.1 · |J|`
   - Действие: уменьшить шаг линейного поиска, увеличить параметр регуляризации `γ`

2. **Застывание (отсутствие прогресса):**
   - Обнаружение: `|J_k - J_{k-50}| / |J_k| < 1e-4` при `||∇J|| > ε_grad`
   - Причина: застревание в седловой точке или плохая обусловленность
   - Действие: применить «толчок» — случайное возмущение параметров с амплитудой `0.01·||q||`

3. **Расходящееся поведение:**
   - Обнаружение: `J_k > 10·J_initial` или `J_k` содержит `NaN/Inf`
   - Действие: прервать оптимизацию, вернуть лучшее найденное решение, выдать предупреждение

### Шаг 4.1.6: Стратегия многократных запусков для избежания локальных минимумов

**Причины необходимости многократных запусков:**
- Невыпуклость функционала из-за барьерных членов создаёт множественные локальные минимумы
- Начальное приближение может привести в «ложную» долину, отделённую барьерами от глобального минимума

**Алгоритм многостартовой оптимизации:**

1. **Генерация набора начальных приближений:**
   - Базовая точка: МНК-инициализация (шаг 4.1.3)
   - Дополнительные точки (3–5 штук):
     * Случайные возмущения базовой точки: `q_k = q_k^base + δ_k`, где `δ_k ~ N(0, 0.2·|q_k^base|)`
     * Точки на границах допустимой области (для исследования крайних решений)
     * Одна точка с нулевыми коэффициентами (`q_k = 0`)

2. **Параллельный запуск оптимизаций:**
   - Запустить независимые оптимизации для каждого начального приближения
   - Для ускорения использовать многопоточность (один поток на запуск)
   - Ограничить общее время: `total_time_limit = 5·timeout` для всех запусков

3. **Выбор лучшего решения:**
   - Сравнить значения функционала всех сходящихся решений
   - Выбрать решение с минимальным `J`
   - При равных значениях предпочесть решение с меньшей нормой градиента (более «глубокий» минимум)

4. **Адаптивное прекращение запусков:**
   - Если текущий лучший результат улучшился менее чем на 1% за последние 3 запуска — прекратить дополнительные запуски
   - Если обнаружено решение с `J < J_threshold` (например, `J_threshold = 0.01·J_initial`) — немедленно завершить все запуски

### Шаг 4.1.7: Адаптивная коррекция параметров в процессе оптимизации

**Динамическая настройка силы барьеров:**
- Наблюдение: при сильных барьерах (`B_j > 1000`) оптимизация может «застрять» вдали от данных
- Стратегия «отжига» барьеров:
  ```
  На итерации k:
      B_j_effective = B_j_initial · max(0.1, 1.0 - 0.01·k)
  ```
  - Постепенное ослабление барьеров позволяет сначала избежать запрещённых областей, затем уточнить аппроксимацию
  - После сходимости выполнить финальную оптимизацию с исходными весами `B_j` для точного выполнения условий

**Адаптивное изменение регуляризации:**
- При обнаружении осцилляций (высокая норма второй производной):
  ```
  если ||F''||_L2 > threshold_oscillation:
      γ_current = min(γ_max, 2.0 · γ_current)
  ```
- При избыточном сглаживании (большие остатки аппроксимации):
  ```
  если J_approx > 10·J_reg:
      γ_current = max(γ_min, 0.5 · γ_current)
  ```

### Шаг 4.1.8: Валидация и постобработка результата оптимизации

**Проверка выполнения всех условий:**
1. **Интерполяционные узлы:**
   - `|F(z_e) - f(z_e)| < 1e-10` для всех `e`
   - При нарушении — применить проекцию: скорректировать свободные параметры для точного выполнения условий

2. **Безопасное расстояние до барьеров:**
   - `|F(y_j) - y_j^*| > ε_safe` для всех `j`
   - При нарушении — увеличить веса `B_j` и повторить оптимизацию с текущего решения как начального приближения

3. **Численная корректность:**
   - Отсутствие `NaN/Inf` в значениях полинома на `[a, b]`
   - Ограниченность нормы коэффициентов: `||q|| < 1e10`

**Сравнение с базовыми решениями:**
- Вычислить функционал для:
  * Чистой интерполяции (`F = P_int`, без коррекции)
  * Взвешенного МНК без отталкивания и регуляризации
- Убедиться, что найденное решение улучшает базовые подходы хотя бы по одному критерию

**Генерация диагностического отчёта:**
```
Результат оптимизации:
  Значение функционала: 5.1942 (улучшение: 87.3% от начального)
  Итераций: 47 из 1000
  Статус: СОШЕЛСЯ (по критерию градиента)
  
  Выполнение условий:
    • Интерполяция: ТОЧНО (макс. отклонение: 2.3e-14)
    • Барьеры: БЕЗОПАСНО (мин. расстояние: 0.123 > 1e-8)
    • Гладкость: НОРМАЛЬНО (||F''|| = 3.21)
  
  Баланс критериев:
    Аппроксимация: 45.2% | Отталкивание: 36.1% | Регуляризация: 18.7%
```

Этот план обеспечивает надёжную, адаптивную и диагностически прозрачную реализацию оптимизационного этапа, учитывающую специфику невыпуклого функционала со сложными барьерными условиями. Подход сочетает теоретически обоснованный выбор алгоритма с практическими стратегиями стабилизации и диагностики проблем сходимости.