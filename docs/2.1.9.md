## Подшаг 2.1.9: Интеграция параметризации с последующими этапами оптимизации — подробный план реализации на C++

### Шаг 2.1.9.1: Проектирование интерфейса функции стоимости для оптимизатора

**Архитектура функтора стоимости:**

1. **Структура `ObjectiveFunctor`:**
   - Хранит константную ссылку на `CompositeParameterization` (параметризацию)
   - Хранит константную ссылку на `OptimizationProblemData` (входные данные задачи)
   - Предоставляет три ключевых метода для оптимизатора:
     * `double value(const std::vector<double>& q)` — вычисление значения функционала
     * `void gradient(const std::vector<double>& q, std::vector<double>& grad)` — вычисление градиента
     * `void value_and_gradient(const std::vector<double>& q, double& f, std::vector<double>& grad)` — комбинированное вычисление (оптимизировано для минимизации повторных операций)

2. **Инкапсуляция данных задачи:**
   - Структура `OptimizationProblemData` содержит:
     * Аппроксимирующие точки: `{x_i, f(x_i), σ_i}` с весами
     * Отталкивающие точки: `{y_j, y_j^*, B_j}` с запрещёнными значениями
     * Интерполяционные узлы: `{z_e, f(z_e)}` (уже учтены в параметризации)
     * Параметр регуляризации `γ`
     * Границы интервала `[a, b]`
   - Все данные передаются по константной ссылке для предотвращения модификации

3. **Стратегия вычисления функционала:**
   - Разложение на три независимых компонента для параллельной обработки:
     ```
     J(q) = J_approx(q) + J_repulse(q) + J_reg(q)
     ```
   - Каждый компонент вычисляется отдельным методом с возможностью отключения:
     * `compute_approximation_term()` — включён всегда при `N_x > 0`
     * `compute_repulsion_term()` — включён при `N_y > 0`
     * `compute_regularization_term()` — включён при `γ > 0`

### Шаг 2.1.9.2: Предварительные вычисления и кэширование для ускорения

**Фаза 1: Кэширование значений компонентов в точках данных**

1. Для аппроксимирующих точек `{x_i}`:
   - Вычислить и сохранить:
     * `P_i = P_int(x_i)` — базисный полином
     * `W_i = W(x_i)` — весовой множитель
     * `target_i = f(x_i)` — целевые значения
     * `weight_i = 1.0 / σ_i` — нормализованные веса
   - Организовать данные в структуру `ApproximationCache` с векторизацией:
     * Данные выровнены по 32-байтной границе для AVX
     * Порядок хранения оптимизирован под последовательный доступ

2. Для отталкивающих точек `{y_j}`:
   - Вычислить и сохранить:
     * `P_j = P_int(y_j)`
     * `W_j = W(y_j)`
     * `forbidden_j = y_j^*` — запрещённые значения
     * `barrier_weight_j = B_j` — веса барьеров
   - Дополнительно сохранить минимально допустимое расстояние `ε_safe = 1e-4` для защиты от деления на ноль

3. Для регуляризационного члена:
   - Сгенерировать узлы квадратуры Гаусса-Лежандра:
     * Базовое число узлов: `n_quad = max(10, 2·n + 1)`
     * Адаптивное уточнение при необходимости
   - Предварительно вычислить:
     * `W_quad[k]`, `W1_quad[k]`, `W2_quad[k]` — весовой множитель и его производные
     * `quad_weight[k]` — веса квадратурной формулы
     * `x_quad[k]` — узлы квадратуры в исходных координатах

**Фаза 2: Кэширование базисных функций**

1. Для каждой точки данных и каждого базисного направления:
   - Вычислить значения базисных функций `φ_k(x)` и их производных до второго порядка
   - Организовать кэш как трёхмерный массив `[точка][базис][производная]`
   - Для базиса Чебышёва использовать рекуррентные соотношения для одновременного вычисления всех порядков

2. Для регуляризационного члена предварительно вычислить матрицу «жёсткости»:
   ```
   K[k][l] = ∫ [φ_k(x)·W(x)]'' · [φ_l(x)·W(x)]'' dx
   ```
   - Вычисление через квадратуру по заранее подготовленным узлам
   - Использование симметрии матрицы для сокращения вычислений в 2 раза
   - Хранение только верхнего треугольника для экономии памяти

**Фаза 3: Оценка вычислительной сложности**

- Без кэширования: каждая оценка функционала требует O((N_x + N_y + N_quad) · n) операций
- С кэшированием: сокращение до O((N_x + N_y) · n_free + N_quad · n_free²) операций
- Ожидаемое ускорение: 10–50× в зависимости от соотношения `m/n` и числа точек

### Шаг 2.1.9.3: Реализация вычисления функционала с защитой от численных аномалий

**Компонент 1: Аппроксимирующий член**

1. Алгоритм вычисления:
   ```
   J_approx = Σ_i weight_i · (P_i + Σ_k q_k·φ_k(x_i)·W_i - target_i)²
   ```
2. Численная защита:
   - Контроль переполнения при вычислении квадратов:
     * Если `|residual| > 1e150` — масштабировать остаток и скорректировать результат
   - Использование компенсированного суммирования (алгоритм Кахана) для повышения точности

**Компонент 2: Отталкивающий член (критически важный)**

1. Алгоритм вычисления с защитой от деления на ноль:
   ```
   для каждой точки y_j:
       F_j = P_j + Σ_k q_k·φ_k(y_j)·W_j
       distance = |forbidden_j - F_j|
       safe_distance = max(distance, ε_safe)  // ε_safe = 1e-4
       term_j = barrier_weight_j / (safe_distance²)
       J_repulse += term_j
       
       // Диагностика опасного приближения к барьеру
       если distance < 5·ε_safe:
           активировать флаг "опасная_близость_к_барьеру"
   ```

2. Адаптивная защита при опасной близости:
   - При активации флага «опасная близость»:
     * Временно уменьшить веса барьера: `B_j_effective = B_j · 0.1`
     * Уменьшить шаг оптимизатора в 2 раза
     * Залогировать событие для диагностики
   - Механизм восстановления:
     * После 5 итераций без приближения к барьеру вернуть исходные веса
     * Постепенно увеличивать шаг оптимизатора

3. Защита от взрывного роста функционала:
   - Максимальное допустимое значение: `J_max = 1e20`
   - При превышении:
     * Вернуть `J_max` вместо реального значения
     * Активировать флаг «функционал_переполнен»
     * Оптимизатор интерпретирует это как сигнал к уменьшению шага

**Компонент 3: Регуляризационный член**

1. Алгоритм вычисления через матрицу жёсткости:
   ```
   J_reg = γ · Σ_k Σ_l q_k · K[k][l] · q_l
   ```
2. Альтернативный метод через квадратуру (при отсутствии матрицы):
   ```
   для каждого узла квадратуры k:
       F2 = P_int''(x_k) + Σ_i q_i · [φ_i(x_k)·W(x_k)]''
       J_reg += γ · quad_weight[k] · F2²
   ```
3. Численная защита:
   - Контроль положительной определённости матрицы `K`:
     * При обнаружении отрицательных собственных значений — добавить диагональную регуляризацию `λ·I`
   - Ограничение максимального значения: `J_reg < 1e15` для предотвращения доминирования

### Шаг 2.1.9.4: Реализация вычисления градиента с аналитическими формулами

**Градиент аппроксимирующего члена:**
```
∂J_approx/∂q_k = 2 · Σ_i weight_i · residual_i · φ_k(x_i) · W_i
где residual_i = (P_i + Σ_l q_l·φ_l(x_i)·W_i - target_i)
```
- Оптимизация: сначала вычислить все `residual_i`, затем для каждого `k` выполнить скалярное произведение
- Векторизация: обработка 4–8 компонент градиента одновременно через SIMD

**Градиент отталкивающего члена:**
```
∂J_repulse/∂q_k = -2 · Σ_j [B_j / distance_j³] · sign(forbidden_j - F_j) · φ_k(y_j) · W_j
где distance_j = max(|forbidden_j - F_j|, ε_safe)
```
- Критическая защита:
  * Ограничение множителя: `factor_j = min(B_j / distance_j³, 1e8)`
  * При `distance_j < ε_safe` использовать субградиент с насыщением
- Адаптивное масштабирование: при активации флага «опасная близость» уменьшить градиент в 10 раз

**Градиент регуляризационного члена:**
```
∂J_reg/∂q_k = 2γ · Σ_l K[k][l] · q_l
```
- Вычисление через матрично-векторное умножение
- При использовании квадратуры: аналитическое дифференцирование подынтегрального выражения

**Комбинированное вычисление значения и градиента:**
- Общая оптимизация: вычислить `F(x_i)` и `F(y_j)` один раз, использовать для обоих компонентов
- Псевдокод:
  ```
  // Шаг 1: вычислить значения F во всех точках
  для всех x_i: F_x[i] = P_i + Σ_k q_k·φ_k(x_i)·W_i
  для всех y_j: F_y[j] = P_j + Σ_k q_k·φ_k(y_j)·W_j
  
  // Шаг 2: вычислить значение функционала
  J = compute_approx_term(F_x) + compute_repulse_term(F_y) + compute_reg_term(q)
  
  // Шаг 3: вычислить градиент
  для каждого k:
      grad[k] = 2·Σ_i weight_i·(F_x[i]-target_i)·φ_k(x_i)·W_i
              - 2·Σ_j factor_j·sign(forbidden_j-F_y[j])·φ_k(y_j)·W_j
              + 2γ·Σ_l K[k][l]·q_l
  ```

### Шаг 2.1.9.5: Адаптация параметров оптимизации на основе характеристик задачи

**Анализ характеристик задачи:**

1. **Масштаб функционала:**
   - Вычислить характерные значения компонентов при нулевых коэффициентах:
     * `J0_approx = J_approx(q=0)`
     * `J0_repulse = J_repulse(q=0)`
     * `J0_reg = J_reg(q=0)`
   - Определить доминирующий компонент и адаптировать параметры оптимизатора

2. **Число обусловленности градиента:**
   - Оценить через конечные разности гессиана в начальной точке
   - При `cond(∇²J) > 1e6` активировать предобуславливание:
     * Диагональное предобуславливание: `q_k_scaled = q_k · sqrt(|∂²J/∂q_k²|)`
     * Или полное предобуславливание через разложение Холецкого малого подпространства

3. **Нелинейность отталкивающего члена:**
   - Оценить вариацию градиента при малых изменениях `q`:
     * Если вариация > 100% — задача сильно нелинейна
     * Рекомендовать уменьшение начального шага оптимизатора в 5–10 раз

**Подбор параметров для конкретных оптимизаторов:**

1. **Для L-BFGS-B:**
   - История кривизны: `m_lbfgs = min(10, n_free / 2)`
   - Начальный шаг: `α0 = 1.0 / max(1.0, ||∇J(q0)||)`
   - Максимальное число итераций: `max_iter = 200 + 10·n_free`
   - Критерий останова по градиенту: `||∇J|| < 1e-6 · max(1.0, J)`

2. **Для Левенберга-Марквардта (при доминировании аппроксимирующего члена):**
   - Начальный параметр демпфирования: `λ0 = 0.01 · max(diag(JᵀJ))`
   - Коэффициент увеличения: `ν_up = 10.0`
   - Коэффициент уменьшения: `ν_down = 0.1`
   - Адаптация: переключаться на Левенберга-Марквардта если `J_approx / J_total > 0.9`

### Шаг 2.1.9.6: Стратегии инициализации коэффициентов перед оптимизацией

**Стратегия 1: Нулевая инициализация (базовая)**
- Установить все `q_k = 0`
- Преимущества: гарантирует выполнение интерполяционных условий, безопасное расстояние от барьеров
- Недостатки: может быть далеко от оптимума при сильном доминировании аппроксимирующего критерия

**Стратегия 2: Инициализация через взвешенный МНК (рекомендуемая)**
1. Решить линейную задачу:
   ```
   минимизировать Σ_i weight_i · (P_i + Σ_k q_k·φ_k(x_i)·W_i - target_i)²
   ```
2. Решение через нормальные уравнения с регуляризацией:
   ```
   (AᵀWA + λI) · q = AᵀW · (target - P)
   где A[i][k] = φ_k(x_i)·W_i, W = diag(weight_i)
   ```
3. Защита от барьеров:
   - После получения решения проверить расстояние до отталкивающих точек
   - Если `min_j |forbidden_j - F(y_j)| < d_safe`:
     * Масштабировать коэффициенты: `q_k = q_k · (d_safe / min_distance) · 0.5`
     * Повторить проверку

**Стратегия 3: Многостартовая инициализация (для сложных ландшафтов)**
1. Сгенерировать 5–10 случайных начальных точек в гиперкубе `[-1, 1]^{n_free}`
2. Для каждой точки выполнить 10 итераций оптимизации с малым шагом
3. Выбрать точку с минимальным значением функционала как стартовую
4. Применять только при `n_free ≤ 10` из-за экспоненциального роста стоимости

**Автоматический выбор стратегии:**
- Если `N_x = 0` (нет аппроксимирующих точек) → Стратегия 1
- Если `N_y = 0` (нет отталкивающих точек) → Стратегия 2 без защиты
- Если `N_y > 0` и `min_j B_j > 100` → Стратегия 1 (сильные барьеры требуют осторожности)
- Иначе → Стратегия 2 с защитой от барьеров

### Шаг 2.1.9.7: Механизмы мониторинга сходимости и диагностики проблем

**Монитор сходимости:**

1. **Критерии сходимости (логическое ИЛИ):**
   - Относительное изменение функционала: `|J_new - J_old| / max(1, J_old) < 1e-8`
   - Норма градиента: `||∇J|| < 1e-6 · max(1, J)`
   - Максимальное число итераций достигнуто
   - Шаг оптимизатора стал меньше порога: `α < 1e-12`

2. **Диагностика проблем сходимости:**
   - Осцилляции функционала:
     * Критерий: `(J_{t} - J_{t-2}) · (J_{t-1} - J_{t-3}) < 0` для 5 последовательных итераций
     * Действие: уменьшить шаг, активировать импульс с затуханием
   - Застывание (плато):
     * Критерий: `|J_{t} - J_{t-10}| / J_{t} < 1e-10` для 20 итераций
     * Действие: перезапустить оптимизацию из текущей точки с увеличенным шагом
   - Расходящаяся траектория:
     * Критерий: `J_{t} > 10 · J_{t-5}` и растущая тенденция
     * Действие: откат к лучшей точке, уменьшение шага в 10 раз

**Журналирование для диагностики:**

1. **Структура записи журнала итерации:**
   ```cpp
   struct OptimizationIterationLog {
       int iteration;
       double objective_value;
       double approx_term, repulse_term, reg_term;  // компоненты функционала
       double gradient_norm;
       double step_size;
       bool barrier_proximity;      // близость к барьеру отталкивания
       bool numerical_anomaly;      // численные аномалии (Inf/NaN)
       std::vector<double> coefficients_snapshot;  // опционально для отладки
   };
   ```

2. **Стратегия сохранения:**
   - Сохранять каждую итерацию в режиме отладки
   - В продакшене: только каждую 10-ю итерацию + все критические события
   - Автоматическое обнаружение паттернов (осцилляции, плато) и генерация рекомендаций

### Шаг 2.1.9.8: Обработка результатов оптимизации и пост-обработка

**Верификация качества решения:**

1. **Проверка интерполяционных условий:**
   - Для всех узлов `z_e`: `|F(z_e) - f(z_e)| < 1e-10`
   - При нарушении: диагностика источника (ошибка в параметризации или численная неустойчивость)

2. **Проверка безопасности относительно барьеров:**
   - Для всех отталкивающих точек: `|F(y_j) - y_j^*| > ε_safe_final = 1e-3`
   - При нарушении: предупреждение «Решение близко к запрещённой области» с рекомендацией увеличить `B_j`

3. **Анализ баланса компонентов функционала:**
   - Идеальный баланс: все компоненты одного порядка величины
   - Дисбаланс:
     * `J_repulse >> J_approx` — слишком сильные барьеры, возможно недоаппроксимация данных
     * `J_approx >> J_repulse` — барьеры неэффективны, возможно нарушение ограничений
     * `J_reg >> J_approx + J_repulse` — избыточная регуляризация, возможно недообучение

**Пост-обработка решения:**

1. **Аналитическая сборка коэффициентов (опционально):**
   - При `n ≤ 20` построить явные коэффициенты `F(x)` для экспорта
   - Проверить точность сборки через сравнение с ленивой оценкой в 10 контрольных точках

2. **Адаптивная коррекция при дисбалансе:**
   - Алгоритм автоматической настройки параметров:
     ```
     если J_repulse / J_approx > 100:
         рекомендовать уменьшить B_j в 10 раз и повторить оптимизацию
     если J_approx / J_repulse > 100 и существуют нарушения барьеров:
         рекомендовать увеличить B_j в 10 раз
     если J_reg / (J_approx + J_repulse) > 10:
         рекомендовать уменьшить γ в 5 раз
     ```

3. **Генерация диагностического отчёта:**
   ```
   ОТЧЁТ ОПТИМИЗАЦИИ
   =================
   Статус: СХОДИМОСТЬ ДОСТИГНУТА (127 итераций)
   
   Значение функционала: J = 3.42e+01
     • Аппроксимация: 2.87e+01 (84.0%)
     • Отталкивание:  4.93e+00 (14.4%)
     • Регуляризация: 5.81e-01 (1.7%)
   
   Качество решения:
     • Макс. ошибка интерполяции: 1.2e-13 ✓
     • Мин. расстояние до барьеров: 2.7e-02 ✓
     • Норма градиента: 3.4e-07 ✓
   
   Рекомендации:
     • Баланс компонентов хороший, параметры оптимальны
     • Для улучшения аппроксимации можно незначительно уменьшить γ
   ```

### Шаг 2.1.9.9: Интеграция с внешними библиотеками оптимизации

**Адаптер для библиотеки NLopt:**

1. Функция обратного вызова для NLopt:
   ```cpp
   double nlopt_objective(unsigned n, const double* q, double* grad, void* data) {
       auto* functor = static_cast<ObjectiveFunctor*>(data);
       std::vector<double> q_vec(q, q + n);
       double f;
       if (grad) {
           std::vector<double> grad_vec(n);
           functor->value_and_gradient(q_vec, f, grad_vec);
           std::copy(grad_vec.begin(), grad_vec.end(), grad);
       } else {
           f = functor->value(q_vec);
       }
       return f;
   }
   ```

2. Настройка оптимизатора NLopt:
   - Алгоритм: `NLOPT_LD_LBFGS` для безусловной оптимизации
   - Параметры: `nlopt_set_ftol_rel(opt, 1e-8)`, `nlopt_set_maxeval(opt, max_iter)`
   - Обработка ошибок: перехват исключений из функтора и преобразование в коды ошибок NLopt

**Адаптер для библиотеки dlib:**

1. Реализация интерфейса `dlib::optimization_function`:
   - Метод `operator()(const column_vector& q)` возвращает значение функционала
   - Метод `derivative(const column_vector& q)` возвращает градиент
   - Преобразование между `std::vector<double>` и `dlib::matrix`

2. Преимущества dlib:
   - Встроенная поддержка линейных ограничений (если потребуется в будущем)
   - Автоматическая адаптация шага на основе истории градиентов
   - Отличная документация и примеры использования

**Стратегия выбора библиотеки:**
- Приоритет 1: собственная реализация L-BFGS для полного контроля и диагностики
- Приоритет 2: NLopt для проверки корректности собственной реализации
- Приоритет 3: dlib для задач с дополнительными ограничениями

Этот план обеспечивает надёжную, эффективную и диагностируемую интеграцию параметризации с этапом оптимизации, включая защиту от численных аномалий, адаптивную настройку параметров, многоуровневый мониторинг сходимости и генерацию подробных диагностических отчётов для принятия обоснованных решений о качестве полученного решения.